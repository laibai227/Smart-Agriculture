from fastapi import FastAPI, UploadFile, Form  # ä» fastapi æ¨¡å—å¯¼å…¥ FastAPI åº”ç”¨ç±»ã€æ–‡ä»¶ä¸Šä¼ ç±»å’Œè¡¨å•å‚æ•°ç±»
from fastapi.responses import JSONResponse  # ä» fastapi.responses æ¨¡å—å¯¼å…¥ JSONResponse ç±»ï¼Œç”¨äºè¿”å› JSON æ ¼å¼çš„ HTTP å“åº”
import chromadb  # å¯¼å…¥ chromadb å‘é‡æ•°æ®åº“åº“ï¼Œç”¨äºçŸ¥è¯†åº“å­˜å‚¨å’Œæ£€ç´¢
import requests  # å¯¼å…¥ requests åº“ï¼Œç”¨äºå‘é€ HTTP è¯·æ±‚ï¼ˆè°ƒç”¨ Ollama åµŒå…¥æœåŠ¡ï¼‰
import uuid  # å¯¼å…¥ uuid åº“ï¼Œç”¨äºç”Ÿæˆå…¨å±€å”¯ä¸€çš„æ–‡æ¡£ ID
import re  # å¯¼å…¥ re æ­£åˆ™è¡¨è¾¾å¼åº“ï¼Œç”¨äºæ–‡æœ¬æ¨¡å¼åŒ¹é…å’Œæå–
import os  # å¯¼å…¥ os æ“ä½œç³»ç»Ÿæ¥å£åº“ï¼Œç”¨äºç¯å¢ƒå˜é‡æ“ä½œ

# ==========================================
# å…¨å±€é…ç½®ï¼šå®šä¹‰æœåŠ¡è¿è¡Œå¸¸é‡å‚æ•°
# ==========================================
os.environ["ANONYMIZED_TELEMETRY"] = "false"  # è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œç¦ç”¨ ChromaDB çš„åŒ¿åé¥æµ‹æ•°æ®æ”¶é›†
OLLAMA_URL = "http://localhost:11434/api/embeddings"  # å®šä¹‰ Ollama åµŒå…¥æœåŠ¡çš„ API åœ°å€
MODEL = "bge-m3"  # å®šä¹‰ä½¿ç”¨çš„åµŒå…¥æ¨¡å‹åç§°
DB_PATH = "./knowledge_db"  # å®šä¹‰ ChromaDB æ•°æ®åº“æ–‡ä»¶å­˜å‚¨è·¯å¾„
DEFAULT_SIMILARITY_THRESHOLD = 0.8  # å®šä¹‰é»˜è®¤ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0.8ï¼‰ï¼Œç”¨äºåˆ¤æ–­çŸ¥è¯†æ˜¯å¦é‡å¤

app = FastAPI(title="Dify MCP Knowledge Service")  # åˆ›å»º FastAPI åº”ç”¨å®ä¾‹ï¼Œè®¾ç½®æœåŠ¡æ ‡é¢˜ä¸º"Dify MCP Knowledge Service"

# ==========================================
# åˆå§‹åŒ– Chromaï¼šè¿æ¥æˆ–åˆ›å»ºå‘é‡æ•°æ®åº“
# ==========================================
client = chromadb.PersistentClient(path=DB_PATH)  # åˆ›å»º Chroma æŒä¹…åŒ–å®¢æˆ·ç«¯ï¼Œè¿æ¥åˆ°æŒ‡å®šè·¯å¾„çš„å‘é‡æ•°æ®åº“
collection = client.get_or_create_collection(name="knowledge_base")  # è·å–æˆ–åˆ›å»ºåä¸º"knowledge_base"çš„çŸ¥è¯†åº“é›†åˆï¼ˆcollectionï¼‰

# ==========================================
# å‘é‡ç”Ÿæˆï¼šå°†æ–‡æœ¬è½¬æ¢ä¸ºåµŒå…¥å‘é‡çš„å‡½æ•°
# ==========================================
def embed_text(text: str):
    """æ¥æ”¶æ–‡æœ¬å­—ç¬¦ä¸²ï¼Œè¿”å›å…¶åµŒå…¥å‘é‡"""
    response = requests.post(OLLAMA_URL, json={"model": MODEL, "prompt": text})  # å‘ Ollama æœåŠ¡å‘é€ POST è¯·æ±‚ï¼Œæºå¸¦æ¨¡å‹åç§°å’Œå¾…è½¬æ¢æ–‡æœ¬
    data = response.json()  # è§£æå“åº”çš„ JSON æ•°æ®
    return data["embedding"]  # ä»å“åº”ä¸­æå–å¹¶è¿”å›åµŒå…¥å‘é‡åˆ—è¡¨

# ==========================================
# è‡ªåŠ¨è¯†åˆ«ä½œç‰©å’Œé˜¶æ®µï¼šä»æ–‡æœ¬ä¸­æå–ä½œç‰©åç§°å’Œç”Ÿé•¿é˜¶æ®µ
# ==========================================
def detect_crop_and_stage(line: str):
    """æ¥æ”¶æ–‡æœ¬è¡Œï¼Œè¿”å›è¯†åˆ«å‡ºçš„ä½œç‰©åç§°å’Œç”Ÿé•¿é˜¶æ®µ"""
    pattern = r"^([\u4e00-\u9fa5A-Za-z0-9ï¼ˆï¼‰()Â·\-\s]+?)\s+([\u4e00-\u9fa5A-Za-z0-9\-ï¼ˆï¼‰()]+æœŸ?)"  # å®šä¹‰æ­£åˆ™æ¨¡å¼ï¼šåŒ¹é…ä½œç‰©åç§°ï¼ˆä¸­æ–‡/è‹±æ–‡/æ•°å­—/æ‹¬å·ç­‰ï¼‰+ ç©ºç™½å­—ç¬¦ + ç”Ÿé•¿é˜¶æ®µï¼ˆä»¥"æœŸ"ç»“å°¾ï¼‰
    match = re.match(pattern, line.strip())  # å»é™¤é¦–å°¾ç©ºç™½åï¼Œä»è¡Œé¦–å¼€å§‹åŒ¹é…æ¨¡å¼
    if match:  # å¦‚æœåŒ¹é…æˆåŠŸ
        crop = match.group(1)  # æå–ç¬¬ä¸€ç»„ï¼šä½œç‰©åç§°
        stage = match.group(2)  # æå–ç¬¬äºŒç»„ï¼šç”Ÿé•¿é˜¶æ®µ
        return crop, stage  # è¿”å›ä½œç‰©å’Œé˜¶æ®µçš„å…ƒç»„
    else:  # å¦‚æœåŒ¹é…å¤±è´¥
        return None, None  # è¿”å› None å…ƒç»„

# ==========================================
# å…ƒæ•°æ®æå–ï¼šä»çŸ¥è¯†æ–‡æœ¬ä¸­æå–ç»“æ„åŒ–å…ƒæ•°æ®
# ==========================================
def extract_metadata(text: str):
    """æ¥æ”¶çŸ¥è¯†æ–‡æœ¬ï¼Œè¿”å›æå–çš„å…ƒæ•°æ®å­—å…¸"""
    lines = text.strip().split("\n")  # å»é™¤é¦–å°¾ç©ºç™½ï¼ŒæŒ‰æ¢è¡Œç¬¦åˆ†å‰²æˆåˆ—è¡¨
    first_line = lines[0] if lines else ""  # è·å–ç¬¬ä¸€è¡Œï¼ˆæ ‡é¢˜è¡Œï¼‰ï¼Œå¦‚æœåˆ—è¡¨ä¸ºç©ºåˆ™ä¸ºç©ºå­—ç¬¦ä¸²
    crop_found, stage_found = detect_crop_and_stage(first_line)  # è°ƒç”¨å‡½æ•°è¯†åˆ«ä½œç‰©å’Œé˜¶æ®µ

    metadata = {"ä½œç‰©": crop_found, "ç”Ÿé•¿é˜¶æ®µ": stage_found}  # åˆ›å»ºå…ƒæ•°æ®å­—å…¸ï¼Œåˆå§‹åŒ–ä½œç‰©å’Œé˜¶æ®µ

    def get_range(pattern):  # å®šä¹‰å†…éƒ¨å‡½æ•°ï¼šä»æ–‡æœ¬ä¸­æå–æ•°å€¼èŒƒå›´
        match = re.search(pattern, text)  # æœç´¢åŒ¹é…æ¨¡å¼
        if match:  # å¦‚æœæ‰¾åˆ°åŒ¹é…
            v1 = float(match.group(1))  # æå–ç¬¬ä¸€ç»„æ•°å€¼å¹¶è½¬ä¸ºæµ®ç‚¹æ•°
            v2 = float(match.group(2)) if match.group(2) else v1  # æå–ç¬¬äºŒç»„æ•°å€¼ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ï¼Œå¦åˆ™ä½¿ç”¨ç¬¬ä¸€ç»„å€¼
            return v1, v2  # è¿”å›æœ€å°å€¼å’Œæœ€å¤§å€¼
        return -1.0, -1.0  # å¦‚æœæœªæ‰¾åˆ°åŒ¹é…ï¼Œè¿”å› -1.0, -1.0

    def get_num(regex):  # å®šä¹‰å†…éƒ¨å‡½æ•°ï¼šä»æ–‡æœ¬ä¸­æå–å•ä¸ªæ•°å€¼
        m = re.search(regex, text)  # æœç´¢åŒ¹é…æ¨¡å¼
        return float(m.group(1)) if m else -1.0  # å¦‚æœæ‰¾åˆ°åˆ™è½¬ä¸ºæµ®ç‚¹æ•°è¿”å›ï¼Œå¦åˆ™è¿”å› -1.0

    min_temp, max_temp = get_range(r"æ¸©åº¦[ï¼š: ]*([0-9]+)[ï½\-â€“]?([0-9]+)?")  # æå–æ¸©åº¦èŒƒå›´
    min_hum, max_hum = get_range(r"æ¹¿åº¦[ï¼š: ]*[^0-9]*([0-9]+)[ï½\-â€“]?([0-9]+)?")  # æå–æ¹¿åº¦èŒƒå›´
    min_soil, max_soil = get_range(r"åœŸå£¤å«æ°´é‡[ï¼š: ]*([0-9]+)[ï½\-â€“]?([0-9]+)?")  # æå–åœŸå£¤å«æ°´é‡èŒƒå›´

    metadata.update({  # æ›´æ–°å…ƒæ•°æ®å­—å…¸ï¼Œæ·»åŠ æ‰€æœ‰æå–çš„æ•°å€¼
        "æœ€ä½æ¸©åº¦": min_temp, "æœ€é«˜æ¸©åº¦": max_temp,
        "æœ€ä½æ¹¿åº¦": min_hum, "æœ€é«˜æ¹¿åº¦": max_hum,
        "æœ€ä½åœŸå£¤å«æ°´é‡": min_soil, "æœ€é«˜åœŸå£¤å«æ°´é‡": max_soil,
        "æ°®è‚¥é‡": get_num(r"æ°®\s*([0-9]+)kg"),  # æå–æ°®è‚¥é‡ï¼ˆkgï¼‰
        "ç£·è‚¥é‡": get_num(r"ç£·\s*([0-9]+)kg"),  # æå–ç£·è‚¥é‡ï¼ˆkgï¼‰
        "é’¾è‚¥é‡": get_num(r"é’¾\s*([0-9]+)kg"),  # æå–é’¾è‚¥é‡ï¼ˆkgï¼‰
        "å…‰ç…§æ—¶é•¿": get_num(r"å…‰ç…§[ï¼š: ]*â‰¥?([0-9]+)h"),  # æå–å…‰ç…§æ—¶é•¿ï¼ˆå°æ—¶ï¼‰
    })
    return metadata  # è¿”å›å®Œæ•´çš„å…ƒæ•°æ®å­—å…¸

# ==========================================
# æŸ¥æ‰¾é‡å¤çŸ¥è¯†ï¼šæŸ¥è¯¢ä¸ç»™å®šå‘é‡æœ€ç›¸ä¼¼çš„æ–‡æ¡£
# ==========================================
def find_similar_docs(embedding):
    """æ¥æ”¶åµŒå…¥å‘é‡ï¼Œè¿”å›æœ€ç›¸ä¼¼çš„ 3 ä¸ªæ–‡æ¡£"""
    results = collection.query(query_embeddings=[embedding], n_results=3)  # åœ¨ ChromaDB ä¸­æŸ¥è¯¢æœ€ç›¸ä¼¼çš„ 3 æ¡è®°å½•
    if not results["ids"] or not results["ids"][0]:  # å¦‚æœæ²¡æœ‰è¿”å›ç»“æœ
        return []  # è¿”å›ç©ºåˆ—è¡¨
    duplicates = []  # åˆ›å»ºç©ºåˆ—è¡¨ï¼Œå­˜å‚¨ç›¸ä¼¼æ–‡æ¡£ä¿¡æ¯
    for i, dist in enumerate(results["distances"][0]):  # éå†è·ç¦»åˆ—è¡¨ï¼ˆè·ç¦»è¶Šå°è¶Šç›¸ä¼¼ï¼‰
        similarity = 1 - float(dist)  # å°†è·ç¦»è½¬æ¢ä¸ºç›¸ä¼¼åº¦ï¼ˆ0-1ï¼Œè¶Šå¤§è¶Šç›¸ä¼¼ï¼‰
        duplicates.append({  # å°†ç›¸ä¼¼æ–‡æ¡£ä¿¡æ¯æ·»åŠ åˆ°åˆ—è¡¨
            "id": results["ids"][0][i],  # æ–‡æ¡£ ID
            "similarity": round(similarity, 3),  # ç›¸ä¼¼åº¦ï¼ˆä¿ç•™ 3 ä½å°æ•°ï¼‰
            "text": results["documents"][0][i],  # æ–‡æ¡£å†…å®¹
        })
    return duplicates  # è¿”å›ç›¸ä¼¼æ–‡æ¡£åˆ—è¡¨

# ==========================================
# ä¸Šä¼ çŸ¥è¯†ï¼ˆè‡ªåŠ¨åˆ†å— + ç›¸ä¼¼åº¦æ£€æµ‹ï¼‰ï¼šHTTP POST æ¥å£
# ==========================================
@app.post("/upload")  # æ³¨å†Œ POST è·¯ç”± /upload
async def upload_knowledge(
    text: str = Form(None),  # æ¥æ”¶è¡¨å•å­—æ®µ textï¼ˆå¯é€‰ï¼Œå­—ç¬¦ä¸²ç±»å‹ï¼‰
    file: UploadFile = None,  # æ¥æ”¶è¡¨å•æ–‡ä»¶ fileï¼ˆå¯é€‰ï¼ŒUploadFile ç±»å‹ï¼‰
    threshold: float = Form(DEFAULT_SIMILARITY_THRESHOLD)  # æ¥æ”¶è¡¨å•å­—æ®µ thresholdï¼ˆå¯é€‰ï¼Œæµ®ç‚¹æ•°ï¼Œé»˜è®¤ 0.8ï¼‰
):
    """å¤„ç†çŸ¥è¯†ä¸Šä¼ è¯·æ±‚ï¼Œæ”¯æŒæ–‡æœ¬æˆ–æ–‡ä»¶ï¼Œè‡ªåŠ¨æ£€æµ‹é‡å¤"""
    if file:  # å¦‚æœæä¾›äº†æ–‡ä»¶
        content = (await file.read()).decode("utf-8")  # å¼‚æ­¥è¯»å–æ–‡ä»¶å­—èŠ‚ç ï¼Œè§£ç ä¸º UTF-8 å­—ç¬¦ä¸²
    elif text:  # å¦‚æœæä¾›äº†æ–‡æœ¬
        content = text  # ç›´æ¥ä½¿ç”¨æ–‡æœ¬å†…å®¹
    else:  # å¦‚æœä¸¤è€…éƒ½æœªæä¾›
        return JSONResponse({"error": "å¿…é¡»æä¾› text æˆ– file"}, status_code=400)  # è¿”å› 400 é”™è¯¯å“åº”

    blocks = [b.strip() for b in re.split(r"\n\s*\n", content) if b.strip()]  # æŒ‰åŒæ¢è¡Œç¬¦åˆ†å‰²å†…å®¹æˆæ®µè½å—ï¼Œå»é™¤é¦–å°¾ç©ºç™½ï¼Œè¿‡æ»¤ç©ºå—
    added_count = 0  # åˆå§‹åŒ–æˆåŠŸæ·»åŠ è®¡æ•°å™¨ä¸º 0

    print(f"\nğŸ§  å½“å‰ç›¸ä¼¼åº¦é˜ˆå€¼ï¼š{threshold}\n")  # æ‰“å°å½“å‰ä½¿ç”¨çš„ç›¸ä¼¼åº¦é˜ˆå€¼

    for block in blocks:  # éå†æ¯ä¸ªçŸ¥è¯†æ®µè½å—
        lines = block.split("\n")  # å°†å—æŒ‰æ¢è¡Œç¬¦åˆ†å‰²æˆè¡Œåˆ—è¡¨
        crop, stage = detect_crop_and_stage(lines[0])  # ä»ç¬¬ä¸€è¡Œè¯†åˆ«ä½œç‰©å’Œé˜¶æ®µ
        if not crop or not stage:  # å¦‚æœä½œç‰©æˆ–é˜¶æ®µæœªè¯†åˆ«æˆåŠŸ
            print(f"âš ï¸ æ— æ³•è¯†åˆ«ä½œç‰©æˆ–é˜¶æ®µï¼š{lines[0]}")  # æ‰“å°è­¦å‘Šä¿¡æ¯
            continue  # è·³è¿‡è¯¥å—ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ª

        emb = embed_text(block)  # å°†å½“å‰æ®µè½å—è½¬æ¢ä¸ºåµŒå…¥å‘é‡
        duplicates = find_similar_docs(emb)  # æŸ¥æ‰¾ç›¸ä¼¼æ–‡æ¡£
        high_sim = [d for d in duplicates if d["similarity"] >= threshold]  # ç­›é€‰ç›¸ä¼¼åº¦å¤§äºç­‰äºé˜ˆå€¼çš„æ–‡æ¡£

        # === âœ… æ–°å¢é€»è¾‘ï¼šè‡ªåŠ¨è·³è¿‡å®Œå…¨ç›¸åŒçš„å†…å®¹ ===
        auto_skip = False  # åˆå§‹åŒ–è‡ªåŠ¨è·³è¿‡æ ‡å¿—ä¸º False
        for d in high_sim:  # éå†é«˜ç›¸ä¼¼åº¦æ–‡æ¡£
            if d["similarity"] == 1.0:  # å¦‚æœç›¸ä¼¼åº¦ä¸º 1.0ï¼ˆå®Œå…¨ç›¸åŒï¼‰
                print(f"\nâ­ï¸ æ£€æµ‹åˆ°å®Œå…¨ç›¸åŒçš„çŸ¥è¯†ï¼ˆ{crop} {stage}ï¼‰ï¼Œå·²è‡ªåŠ¨è·³è¿‡ï¼š\n{d['text']}\n")  # æ‰“å°è‡ªåŠ¨è·³è¿‡ä¿¡æ¯
                auto_skip = True  # è®¾ç½®è‡ªåŠ¨è·³è¿‡æ ‡å¿—ä¸º True
                break  # è·³å‡ºå¾ªç¯
        if auto_skip:  # å¦‚æœéœ€è¦è‡ªåŠ¨è·³è¿‡
            continue  # è·³è¿‡å½“å‰å—ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ª
        # ==================== è‡ªåŠ¨è·³è¿‡é€»è¾‘ç»“æŸ

        if high_sim:  # å¦‚æœå­˜åœ¨é«˜ç›¸ä¼¼åº¦æ–‡æ¡£ï¼ˆä½†éå®Œå…¨ç›¸åŒï¼‰
            print(f"\nâš ï¸ æ£€æµ‹åˆ°ç›¸ä¼¼çŸ¥è¯†ç‰‡æ®µï¼ˆ{crop} {stage}ï¼‰:")  # æ‰“å°ç›¸ä¼¼æ£€æµ‹è­¦å‘Š
            for d in high_sim:  # éå†é«˜ç›¸ä¼¼åº¦æ–‡æ¡£
                print(f"ç›¸ä¼¼åº¦: {d['similarity']} | å·²å­˜åœ¨å†…å®¹:\n{d['text']}\n")  # æ‰“å°ç›¸ä¼¼åº¦å’Œå·²å­˜åœ¨å†…å®¹
            print(f"ğŸ†• æ–°ä¸Šä¼ å†…å®¹:\n{block}\n")  # æ‰“å°æ–°å†…å®¹
            choice = input("æ˜¯å¦ä»ç„¶ä¸Šä¼ æ­¤å†…å®¹ï¼Ÿ(y/n): ").strip().lower()  # äº¤äº’å¼è¯¢é—®ç”¨æˆ·æ˜¯å¦ç»§ç»­ä¸Šä¼ 
            if choice != "y":  # å¦‚æœç”¨æˆ·è¾“å…¥ä¸æ˜¯ y
                print("â¹ï¸ å·²è·³è¿‡æ­¤ç‰‡æ®µã€‚\n")  # æ‰“å°è·³è¿‡ä¿¡æ¯
                continue  # è·³è¿‡å½“å‰å—ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ª

        doc_id = str(uuid.uuid4())  # ç”Ÿæˆå…¨å±€å”¯ä¸€çš„ UUID ä½œä¸ºæ–‡æ¡£ ID
        metadata = extract_metadata(block)  # ä»æ®µè½å—æå–å…ƒæ•°æ®
        metadata["å”¯ä¸€ç¼–å·"] = doc_id  # å°† UUID æ·»åŠ åˆ°å…ƒæ•°æ®
        collection.add(ids=[doc_id], embeddings=[emb], documents=[block], metadatas=[metadata])  # å°†æ–‡æ¡£æ·»åŠ åˆ° ChromaDBï¼ˆåŒ…å« IDã€å‘é‡ã€åŸæ–‡ã€å…ƒæ•°æ®ï¼‰
        print(f"âœ… æˆåŠŸæ·»åŠ çŸ¥è¯† [{doc_id}]ï¼ˆ{crop} {stage}ï¼‰")  # æ‰“å°æˆåŠŸæ·»åŠ ä¿¡æ¯
        added_count += 1  # æˆåŠŸè®¡æ•°å™¨åŠ  1

    return {"status": "success", "message": f"æˆåŠŸæ·»åŠ  {added_count} æ¡çŸ¥è¯†"}  # è¿”å› JSON å“åº”ï¼ŒåŒ…å«æˆåŠŸçŠ¶æ€å’Œæ·»åŠ æ•°é‡

# ==========================================
# æœç´¢æ¥å£ï¼šHTTP POST æ¥å£
# ==========================================
@app.post("/search")  # æ³¨å†Œ POST è·¯ç”± /search
async def search_knowledge(query: str = Form(...), top_k: int = Form(3)):  # æ¥æ”¶å¿…å¡«çš„æŸ¥è¯¢å­—ç¬¦ä¸²å’Œå¯é€‰çš„è¿”å›æ•°é‡ï¼ˆé»˜è®¤3ï¼‰
    """çŸ¥è¯†æœç´¢æ¥å£ï¼Œæ¥æ”¶æŸ¥è¯¢ï¼Œè¿”å›æœ€ç›¸ä¼¼çš„ top_k æ¡çŸ¥è¯†"""
    query_emb = embed_text(query)  # å°†æŸ¥è¯¢æ–‡æœ¬è½¬æ¢ä¸ºåµŒå…¥å‘é‡
    results = collection.query(query_embeddings=[query_emb], n_results=top_k)  # æŸ¥è¯¢æœ€ç›¸ä¼¼çš„ top_k æ¡è®°å½•
    data = []  # åˆ›å»ºç©ºåˆ—è¡¨ï¼Œå­˜å‚¨æ ¼å¼åŒ–ç»“æœ
    for i in range(len(results["ids"][0])):  # éå†æŸ¥è¯¢ç»“æœ
        data.append({  # æ ¼å¼åŒ–æ¯æ¡ç»“æœ
            "id": results["ids"][0][i],  # æ–‡æ¡£ ID
            "text": results["documents"][0][i],  # æ–‡æ¡£å†…å®¹
            "metadata": results["metadatas"][0][i],  # å…ƒæ•°æ®
            "score": float(results["distances"][0][i])  # ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆè·ç¦»ï¼‰
        })
    return {"count": len(data), "data": data}  # è¿”å›åŒ…å«æ•°é‡å’Œæ•°æ®çš„ JSON å“åº”

# ==========================================
# æŸ¥çœ‹æ‰€æœ‰çŸ¥è¯†ï¼šHTTP GET æ¥å£
# ==========================================
@app.get("/list")  # æ³¨å†Œ GET è·¯ç”± /list
async def list_knowledge():
    """è¿”å›çŸ¥è¯†åº“ä¸­æ‰€æœ‰çŸ¥è¯†æ¡ç›®"""
    results = collection.get(include=["documents", "metadatas"])  # è·å–é›†åˆä¸­æ‰€æœ‰æ–‡æ¡£å’Œå…ƒæ•°æ®
    data = [  # åˆ—è¡¨æ¨å¯¼å¼ï¼Œæ ¼å¼åŒ–æ‰€æœ‰æ–‡æ¡£
        {"id": results["ids"][i], "text": results["documents"][i], "metadata": results["metadatas"][i]}  # æ¯æ¡åŒ…å« IDã€å†…å®¹ã€å…ƒæ•°æ®
        for i in range(len(results["documents"]))  # éå†æ‰€æœ‰æ–‡æ¡£
    ]
    return {"count": len(data), "data": data}  # è¿”å›åŒ…å«æ•°é‡å’Œå®Œæ•´æ•°æ®çš„ JSON å“åº”

# ==========================================
# æŒ‰ ID æŸ¥è¯¢å•æ¡çŸ¥è¯†ï¼šHTTP GET æ¥å£
# ==========================================
@app.get("/get/{doc_id}")  # æ³¨å†Œ GET è·¯ç”± /get/{doc_id}ï¼Œdoc_id æ˜¯è·¯å¾„å‚æ•°
async def get_doc(doc_id: str):  # æ¥æ”¶æ–‡æ¡£ ID å­—ç¬¦ä¸²å‚æ•°
    """
    æ ¹æ®å”¯ä¸€ç¼–å·æŸ¥è¯¢å•æ¡çŸ¥è¯†
    """
    try:  # å°è¯•æŸ¥è¯¢æ–‡æ¡£
        raw = collection.get(ids=[doc_id], include=["documents", "metadatas"])  # æ ¹æ® ID æŸ¥è¯¢æ–‡æ¡£
        if not raw["ids"]:  # å¦‚æœè¿”å›çš„ ID åˆ—è¡¨ä¸ºç©ºï¼ˆæ–‡æ¡£ä¸å­˜åœ¨ï¼‰
            return JSONResponse({"status": "not_found", "error": f"ID {doc_id} ä¸å­˜åœ¨"}, status_code=404)  # è¿”å› 404 é”™è¯¯å“åº”

        return {  # è¿”å›æˆåŠŸå“åº”å’Œæ–‡æ¡£å†…å®¹
            "status": "success",
            "id": raw["ids"][0],  # æ–‡æ¡£ ID
            "text": raw["documents"][0],  # æ–‡æ¡£å†…å®¹
            "metadata": raw["metadatas"][0]  # å…ƒæ•°æ®
        }
    except Exception as e:  # å¦‚æœæŸ¥è¯¢è¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸
        return JSONResponse({"status": "failed", "error": str(e)}, status_code=500)  # è¿”å› 500 é”™è¯¯å“åº”

# ==========================================
# åˆ é™¤ï¼šHTTP DELETE æ¥å£
# ==========================================
@app.delete("/delete")  # æ³¨å†Œ DELETE è·¯ç”± /delete
async def delete_doc(doc_id: str):  # æ¥æ”¶æŸ¥è¯¢å‚æ•° doc_id
    """æ ¹æ® ID åˆ é™¤å•æ¡çŸ¥è¯†"""
    try:  # å°è¯•åˆ é™¤
        collection.delete(ids=[doc_id])  # ä» ChromaDB ä¸­åˆ é™¤æŒ‡å®š ID çš„æ–‡æ¡£
        return {"status": "success", "deleted_id": doc_id}  # è¿”å›æˆåŠŸçŠ¶æ€å’Œåˆ é™¤çš„ ID
    except Exception as e:  # å¦‚æœåˆ é™¤å¤±è´¥
        return {"status": "failed", "error": str(e)}  # è¿”å›å¤±è´¥çŠ¶æ€å’Œé”™è¯¯ä¿¡æ¯

# ==========================================
# æ¸…ç©ºçŸ¥è¯†åº“ï¼šHTTP DELETE æ¥å£
# ==========================================
@app.delete("/clear")  # æ³¨å†Œ DELETE è·¯ç”± /clear
async def clear_knowledge():
    """æ¸…ç©ºçŸ¥è¯†åº“ä¸­æ‰€æœ‰çŸ¥è¯†"""
    try:  # å°è¯•æ¸…ç©º
        all_ids = collection.get()["ids"]  # è·å–æ‰€æœ‰æ–‡æ¡£ ID
        if all_ids:  # å¦‚æœ ID åˆ—è¡¨ä¸ä¸ºç©ºï¼ˆçŸ¥è¯†åº“æœ‰å†…å®¹ï¼‰
            collection.delete(ids=all_ids)  # åˆ é™¤æ‰€æœ‰ ID å¯¹åº”çš„æ–‡æ¡£
            return {"status": "success", "message": f"å·²åˆ é™¤ {len(all_ids)} æ¡çŸ¥è¯†"}  # è¿”å›æˆåŠŸå’Œåˆ é™¤æ•°é‡
        else:  # å¦‚æœ ID åˆ—è¡¨ä¸ºç©º
            return {"status": "success", "message": "çŸ¥è¯†åº“ä¸ºç©º"}  # è¿”å›æˆåŠŸå’Œç©ºåº“æç¤º
    except Exception as e:  # å¦‚æœæ¸…ç©ºå¤±è´¥
        return {"status": "failed", "error": str(e)}  # è¿”å›å¤±è´¥çŠ¶æ€å’Œé”™è¯¯ä¿¡æ¯

# ==========================================
# å¥åº·æ£€æŸ¥ï¼šHTTP GET æ¥å£
# ==========================================
@app.get("/health")  # æ³¨å†Œ GET è·¯ç”± /health
async def health_check():
    """æœåŠ¡å¥åº·æ£€æŸ¥æ¥å£ï¼Œç”¨äºç›‘æ§"""
    return {"status": "ok"}  # è¿”å›å¥åº·çŠ¶æ€

# ==========================================
# å¯åŠ¨ï¼šä¸»å…¥å£
# ==========================================
if __name__ == "__main__":  # å¦‚æœç›´æ¥è¿è¡Œæ­¤è„šæœ¬ï¼ˆè€Œéè¢«å¯¼å…¥ï¼‰
    import uvicorn  # å¯¼å…¥ uvicorn ASGI æœåŠ¡å™¨
    print("ğŸš€ MCP çŸ¥è¯†åº“æœåŠ¡å·²å¯åŠ¨ (http://localhost:8450)")  # æ‰“å°æœåŠ¡å¯åŠ¨æç¤º
    uvicorn.run(app, host="0.0.0.0", port=8450)  # å¯åŠ¨ uvicorn æœåŠ¡å™¨ï¼Œç›‘å¬æ‰€æœ‰ç½‘ç»œæ¥å£çš„ 8450 ç«¯å£ï¼Œè¿è¡Œ FastAPI åº”ç”¨